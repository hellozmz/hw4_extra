{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b7476a",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4 Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d459",
   "metadata": {},
   "source": [
    "This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a5c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
      "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-yql24nld\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-yql24nld\n",
      "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pybind11 in /opt/conda/envs/train/lib/python3.11/site-packages (2.13.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9fb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /opt/conda/envs/train/lib/python3.11/site-packages/pybind11/include (found version \"2.13.6\")\n",
      "-- Found cuda, building cuda backend\n",
      "Fri Dec 27 03:05:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             99W /  700W |      11MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:2A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             69W /  700W |     129MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             69W /  700W |     123MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             68W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9A:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             69W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:AB:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             69W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:BA:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             68W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             70W /  700W |       3MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX 8.6+PTX\n",
      "-- Configuring done (2.1s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /mnt/zhumingzhu/work/00test/hw4_extra/build\n",
      "make[1]: Entering directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "make[2]: Entering directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "make[3]: Entering directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "make[3]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "make[3]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n",
      "make[1]: Leaving directory '/mnt/zhumingzhu/work/00test/hw4_extra/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45349235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54d7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5945207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PTB dataset\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea5c0a",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2f639",
   "metadata": {},
   "source": [
    "In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks. \n",
    "\n",
    "You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n",
    "\n",
    "Transformers are composed of three mains components that you will implement. \n",
    "1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence. \n",
    "2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep. \n",
    "3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n",
    "\n",
    "![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
    "\n",
    "The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094ff30",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the Multi-Head Attention Activation Layer\n",
    "\n",
    "In this subproblem, you will be implementing the `forward` function of a \"base\" attention activation layer `MultiHeadAttention` in `python/needle/nn/nn_transformer.py`. This activation layer will take in three inputs: \n",
    "<p style=\"text-align: center;\">multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$, keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$</p>\n",
    "\n",
    "where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the sequence length, and $D$ is the hidden dimension. \n",
    "\n",
    "The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as follows:\n",
    "\n",
    "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$</p>\n",
    "\n",
    "Note that the matrix multiplications above are batched. This functionality is not natively supported in needle yet, so we have provided a convenient function `matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal in this section is to return $X$ given the input queries, keys, and values. \n",
    "\n",
    "For auto-regressive Transformer, this attention should support causal masking using the function `self.create_causal_mask` we have provided. This is to make sure that the prediction of next token only depends on it's previous tokens. Specifically, causal masking is applying a mask before the softmax so that the softmax probability is computed over a masked matrix of $\\frac{Q K^T}{\\sqrt{D}}$. \n",
    "\n",
    "In addition, your implementation should apply dropout to the attention softmax $\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout` function of the `MultiHeadAttention` module.\n",
    "\n",
    "Importantly, this layer is only an activation function, and has no trainable variables (these come later).\n",
    "\n",
    "Once you have finished your implementation, test your code with the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ee3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n",
      "DEBUG: Using backend nd\n",
      "['Any', 'AttentionLayer', 'BatchNorm1d', 'BatchNorm2d', 'Callable', 'Conv', 'Conv_BK', 'Dropout', 'Embedding', 'Flatten', 'Identity', 'LSTM', 'LSTMCell', 'LayerNorm1d', 'Linear', 'List', 'Module', 'MultiHeadAttention', 'Parameter', 'RNN', 'RNNCell', 'ReLU', 'Residual', 'Sequential', 'Sigmoid', 'SoftmaxLoss', 'Tensor', 'Transformer', 'TransformerLayer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'init', 'math', 'ndarray', 'nn_basic', 'nn_conv', 'nn_sequence', 'nn_transformer', 'np', 'ops']\n",
      "/mnt/zhumingzhu/work/00test/hw4_extra/./python/needle/nn/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import needle.nn\n",
    "print(dir(needle.nn))\n",
    "print(needle.nn.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7eeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4_extra/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4_extra\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 112 items / 96 deselected / 16 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-False-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7faac6dbc210>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7faac6dbc210>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...8e-02]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7faac6dbc210>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...50863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7faac6dbf950>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7faac6dbf950>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...7850863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c8147c90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-False-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6371c10>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6371c10>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...4e+00]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6371c10>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...69881e-06]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6371b90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6371b90>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...4269881e-06]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6372410>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-True-64-31-5-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c629f810>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c629f810>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0....  [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cpu())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c629f810>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c629f850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c629f850>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c629d110>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.0-True-64-31-5-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6391890>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6391890>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0....  [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cpu())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6391890>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6390310>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6390310>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c7a6ff50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-False-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6124450>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6124450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...8e-02]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6124450>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...50863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6125050>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6125050>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...7850863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7faac6cb8090>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-False-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6150450>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6150450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...4e+00]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6150450>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...69881e-06]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6150390>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6150390>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661099e-04 ... 1.84552651e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...4269881e-06]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c61510d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-True-64-31-5-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6373f50>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6373f50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0....  [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cpu())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6373f50>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6371750>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6371750>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c76501d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cpu-0.1-True-64-31-5-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6339450>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6339450>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0....  [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cpu())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6339450>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6338310>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6338310>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [3.79857753e-04 3.32673735e-05 1.02457634e-04 ... 1.88220401e-05\n",
      "    1.46508346e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6338590>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.0-False-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6201cd0>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6201cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...8e-02]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6201cd0>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...50863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6201410>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6201410>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...7850863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6203350>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.0-False-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729ee90>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729ee90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...4e+00]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729ee90>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...69881e-06]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c729c9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c729c9d0>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...4269881e-06]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c729d4d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.0-True-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c7250710>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c7250710>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0.... [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cuda())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c7250710>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c72508d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c72508d0>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c7250150>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.0-True-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dropout    = 0.0\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729f850>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729f850>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0.... [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cuda())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c729f850>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c729cb50>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c729cb50>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 1.0\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6219250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.1-False-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6288bd0>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6288bd0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...8e-02]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6288bd0>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...50863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c628b6d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c628b6d0>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...7850863e-04]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c628b710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m_____________ test_attention_activation[cuda-0.1-False-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = False\n",
      "dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c62154d0>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c62154d0>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -5.80595851e-01 -3.82937253e-01 ...  6.41318798e-01\n",
      "    -1.12713397e+00 -8.56185425e...4e+00]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c62154d0>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      "...69881e-06]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6215590>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6215590>\n",
      "        x          = needle.Tensor([[[[9.53209758e-01 5.43812872e-04 6.62661158e-04 ... 1.84552663e-03\n",
      "    3.14840872e-04 9.63564147e-04]\n",
      " ...4269881e-06]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6214210>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.1-True-64-31-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6324a90>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...594168e-02,  1.14688802e+00, ...,\n",
      "          -6.52230859e-01,  1.54555345e+00, -5.07373631e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6324a90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 4\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0.... [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cuda())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [-3.02162170e-02  7.36534357e-01  1.22386324e+00 ... -1.10687768e+00\n",
      "     1.97228938e-02  6.94004774e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c6324a90>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...1e+00]\n",
      "   [-1.45907298e-01  9.85594168e-02  1.14688802e+00 ... -6.52230859e-01\n",
      "     1.54555345e+00 -5.07373631e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6324610>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c6324610>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [9.08775371e-04 1.95637532e-03 3.18490365e-03 ... 3.09648080e-04\n",
      "    9.55311174e-04 9.67394769e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (4, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c6324b90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 1. 1. ... 0. 1. 1.]\n",
      "   [0. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 0. 0. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[31m\u001b[1m______________ test_attention_activation[cuda-0.1-True-64-31-5-8] ______________\u001b[0m\n",
      "\n",
      "batch_size = 8, num_heads = 5, queries_len = 31, inner_dim = 64, causal = True\n",
      "dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_heads\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mqueries_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m31\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minner_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_activation\u001b[39;49;00m(batch_size, num_heads, queries_len, inner_dim, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, num_heads,\u001b[90m\u001b[39;49;00m\n",
      "            queries_len, inner_dim).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        layer = nn.MultiHeadAttention(\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       result, probs = layer(\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "            ndl.Tensor(q, device=device),\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dropout    = 0.1\n",
      "inner_dim  = 64\n",
      "layer      = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c615ee90>\n",
      "num_heads  = 5\n",
      "q          = array([[[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00, ...,\n",
      "          -9.72985864e-01,  8.51403356e-01,  5.37231...578935e+00,  5.42450488e-01, ...,\n",
      "           1.22951768e-01,  2.24718475e+00, -9.15048599e-01]]]],\n",
      "      dtype=float32)\n",
      "queries_len = 31\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:43: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565...e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]]))\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c615ee90>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:119: in forward\n",
      "    \u001b[0mprobs = \u001b[96mself\u001b[39;49;00m.dropout(\u001b[96mself\u001b[39;49;00m.softmax(qk)) \u001b[90m# (bs, head, queries_len, keys_values_len)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        _          = 31\n",
      "        batch_size = 8\n",
      "        k          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        k_dim      = 64\n",
      "        keys_values_len = 31\n",
      "        mask       = NDArray([[[[-0.0000000e+00 -3.4028235e+38 -3.4028235e+38 ... -3.4028235e+38\n",
      "    -3.4028235e+38 -3.4028235e+38]\n",
      "   [-0.... [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00 ... -0.0000000e+00\n",
      "    -0.0000000e+00 -0.0000000e+00]]]], device=cuda())\n",
      "        num_head   = 5\n",
      "        probs      = None\n",
      "        q          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        q_dim      = 64\n",
      "        qk         = needle.Tensor([[[[ 6.88838911e+00 -3.40282347e+38 -3.40282347e+38 ... -3.40282347e+38\n",
      "    -3.40282347e+38 -3.40282347e...7e+38]\n",
      "   [ 1.60713410e+00 -8.28085482e-01  2.96785861e-01 ... -1.39763486e+00\n",
      "    -1.64816594e+00  9.47970772e+00]]]])\n",
      "        queries_len = 31\n",
      "        result     = None\n",
      "        self       = <needle.nn.nn_transformer.MultiHeadAttention object at 0x7fa5c615ee90>\n",
      "        v          = needle.Tensor([[[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 ... -9.72985864e-01\n",
      "     8.51403356e-01  5.37231565e...2e-01]\n",
      "   [ 4.44086164e-01  1.68578935e+00  5.42450488e-01 ...  1.22951768e-01\n",
      "     2.24718475e+00 -9.15048599e-01]]]])\n",
      "        v_dim      = 64\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:74: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "...00000e+00]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]]),)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c615f190>\n",
      "\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:241: in forward\n",
      "    \u001b[0mdrop_matrix = init.randb(*x.shape, p=(\u001b[94m1\u001b[39;49;00m-\u001b[96mself\u001b[39;49;00m.p))\u001b[90m\u001b[39;49;00m\n",
      "        self       = <needle.nn.nn_basic.Dropout object at 0x7fa5c615f190>\n",
      "        x          = needle.Tensor([[[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      " ...0000000e+00]\n",
      "   [3.79857724e-04 3.32673735e-05 1.02457627e-04 ... 1.88220401e-05\n",
      "    1.46508355e-05 9.96865094e-01]]]])\n",
      "\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:47: in randb\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        p          = 0.9\n",
      "        requires_grad = False\n",
      "        shape      = (8, 5, 31, 31)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:223: in __init__\n",
      "    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        array      = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        kwargs     = {}\n",
      "        requires_grad = False\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fa5c615d450>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:236: in _array_from_numpy\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n",
      "        device     = cpu()\n",
      "        dtype      = 'bool'\n",
      "        numpy_array = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "a = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "dtype = 'bool', device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92marray\u001b[39;49;00m(a, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Convenience methods to match numpy a bit more closely.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        dtype = \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m dtype\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m dtype == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[[1. 0. 1. ... 1. 0. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1....... 1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]\n",
      "   [1. 1. 1. ... 1. 1. 1.]]]], device=cpu())\n",
      "device     = cpu()\n",
      "dtype      = 'bool'\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:662: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-False-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-False-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-True-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.0-True-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-False-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-False-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-True-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cpu-0.1-True-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-False-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-False-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-True-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.0-True-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-False-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-False-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-True-64-31-5-4]\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_activation[cuda-0.1-True-64-31-5-8]\u001b[0m - AssertionError\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m16 failed\u001b[0m, \u001b[33m96 deselected\u001b[0m\u001b[31m in 5.99s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_activation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65aea6",
   "metadata": {},
   "source": [
    "## Part 2 Implementing the Self-Attention Layer with trainable parameters\n",
    "\n",
    "In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`. \n",
    "\n",
    "This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n",
    "\n",
    "Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n",
    "\n",
    "(1) map queries, key, and values to heads.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n",
    "\n",
    "where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n",
    "\n",
    "(2) unravel heads from the channels axis.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n",
    "\n",
    "(3) compute the multi-head attention activation.\n",
    "\n",
    "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n",
    "\n",
    "The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n",
    "\n",
    "(4) project back to the input space of the layer with `self.out_projection`\n",
    "\n",
    "<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n",
    "\n",
    "Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n",
    "\n",
    "Once finished, you may test your layer with the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b2fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4_extra/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4_extra\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-False-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-False-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-False-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-False-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.0-True-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.0-True-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-True-32-8-27-11-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.0-True-32-8-27-11-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-False-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.1-True-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m________________ test_attention_layer[cpu-0.1-True-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-True-32-8-27-11-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cpu-0.1-True-32-8-27-11-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-False-32-8-27-5-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-False-32-8-27-5-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.0-False-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.0-False-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.0-True-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-False-32-8-27-5-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-False-32-8-27-5-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.1-False-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m______________ test_attention_layer[cuda-0.1-False-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-5-4] ________________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n",
      "         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n",
      "          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n",
      "         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n",
      "         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-5-8] ________________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n",
      "         -0.37128997,  0.06527077],\n",
      "        [-1.0428...\n",
      "        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n",
      "          0.526678  ,  0.26005384]]], dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "seq_len    = 5\n",
      "v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n",
      "         -0.7493343 , -0.15542848],\n",
      "        [ 2.5435...\n",
      "        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n",
      "          0.08334691,  0.17072625]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-11-4] _______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n",
      "         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n",
      "          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n",
      "          1.8853118 , -0.61863774],\n",
      "        [-0.3089...\n",
      "        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n",
      "          0.8674379 ,  1.4639174 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[31m\u001b[1m_______________ test_attention_layer[cuda-0.1-True-32-8-27-11-8] _______________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "input_dim  = 27\n",
      "k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n",
      "          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n",
      "         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n",
      "      dtype=float32)\n",
      "num_head   = 8\n",
      "q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "seq_len    = 11\n",
      "v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n",
      "          0.3303766 , -0.71756196],\n",
      "        [-1.2459...\n",
      "        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n",
      "          1.0845269 ,  0.7676009 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-False-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.0-True-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-False-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cpu-0.1-True-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'AttentionLayer'\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m32 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 2.15s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8fb30",
   "metadata": {},
   "source": [
    "## Part 3 Implementing a prenorm residual Transformer Layer\n",
    "\n",
    "You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class. \n",
    "\n",
    "You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n",
    "\n",
    "x - current sequence of hidden states\n",
    "\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n",
    "\n",
    "For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n",
    "- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n",
    "- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n",
    "\n",
    "Once finished, run the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e0fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4_extra/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4_extra\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] ______________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] ______________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] ______________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] ____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] ____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] ____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] ____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  -0.7593835  -0.6427555\n",
      "   -1.0407752   0.9083351   0.4292678   0....46 -0.24666895  1.4582475\n",
      "    0.32713595 -1.6861233   0.603675    1.5231946   0.31856778\n",
      "    0.08265579 -0.7877033 ]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n",
      "         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n",
      "          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m_____________ test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -2.76626378e-01 -4.02163118e-02  5.16363382e-01 -4.79716718e-01\n",
      "    7.03656971e-01  1.89074099e-01 -1.07197821e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 5\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n",
      "          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] _____________\u001b[0m\n",
      "\n",
      "batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 2\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-8.04154854e-03  6.31518424e-01  2.25275445e+00 -7.59383500e-01\n",
      "   -6.42755508e-01 -1.04077518e+00  9... -4.08079267e-01  2.06604743e+00 -7.85944700e-01  2.83783585e-01\n",
      "    6.31931484e-01  3.06893229e-01 -1.25974321e+00]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n",
      "         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n",
      "          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] _____________\u001b[0m\n",
      "\n",
      "batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32\n",
      "hidden_size = 64, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       layer = nn.TransformerLayer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, num_head, dim_head, hidden_size,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\u001b[0m\n",
      "\n",
      "batch_size = 4\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...63     0.43104777\n",
      "    0.93574804]\n",
      "  [ 3.4161992   0.04355975 -0.76129997 ...  0.59368753  0.31795904\n",
      "    0.14328356]]])\n",
      "num_head   = 8\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n",
      "          0.31795904,  0.14328356]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:134: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-False-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-False-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-False-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-False-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-True-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-True-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-True-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.0-True-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-False-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-False-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-False-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-False-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-True-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-True-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-True-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cpu-0.1-True-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-5-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-5-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-11-2]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-11-4]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'TransformerLayer'\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m32 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 3.55s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e78953",
   "metadata": {},
   "source": [
    "## Part 4 Implementing the Transformer model\n",
    "\n",
    "In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`. \n",
    "\n",
    "As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n",
    "\n",
    "The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings. \n",
    "\n",
    "To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id. \n",
    "\n",
    "Last, add the created positional encoding to the input token embeddings before your transformer layers.\n",
    "\n",
    "Once complete, submit the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5fb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /opt/conda/envs/train/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/mnt/zhumingzhu/work/00test/hw4_extra/.hypothesis/examples'))\n",
      "rootdir: /mnt/zhumingzhu/work/00test/hw4_extra\n",
      "plugins: hypothesis-6.115.5\n",
      "collected 112 items / 80 deselected / 32 selected                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] _____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cpu()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.0\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] ___________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = False\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 2\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m____________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...6776533  0.4178168\n",
      "   -1.1135957 ]\n",
      "  [ 0.6823758  -0.6566193   0.32996404 ...  1.2692264   1.2295368\n",
      "   -0.991682  ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 5\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n",
      "          1.2295368 , -0.991682  ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[31m\u001b[1m___________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] ____________\u001b[0m\n",
      "\n",
      "batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4\n",
      "num_head = 8, dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n",
      "            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n",
      "            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n",
      "        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       model = nn.Transformer(\u001b[90m\u001b[39;49;00m\n",
      "            input_dim, hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n",
      "            num_head=num_head,\u001b[90m\u001b[39;49;00m\n",
      "            dim_head=dim_head,\u001b[90m\u001b[39;49;00m\n",
      "            dropout=dropout,\u001b[90m\u001b[39;49;00m\n",
      "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
      "            device=device,\u001b[90m\u001b[39;49;00m\n",
      "            batch_first=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.nn' has no attribute 'Transformer'\u001b[0m\n",
      "\n",
      "batch_size = 8\n",
      "causal     = True\n",
      "device     = cuda()\n",
      "dim_head   = 32\n",
      "dropout    = 0.1\n",
      "hidden_size = 64\n",
      "input_dim  = 27\n",
      "ndl_x      = needle.Tensor([[[-0.00804155  0.6315184   2.2527544  ...  0.57792866  1.2252008\n",
      "   -1.2465482 ]\n",
      "  [ 1.8416243   0.6522...5524445 -1.2543415\n",
      "    0.92202413]\n",
      "  [ 0.07931643  0.34981483  1.627947   ...  0.13004941  1.3878478\n",
      "    0.7909228 ]]])\n",
      "num_head   = 8\n",
      "num_layers = 4\n",
      "seq_len    = 11\n",
      "x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n",
      "          1.2252008 , -1.2465482 ],\n",
      "        [ 1.8416...\n",
      "        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n",
      "          1.3878478 ,  0.7909228 ]]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:182: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8]\u001b[0m - AttributeError: module 'needle.nn' has no attribute 'Transformer'\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m32 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 3.15s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899683fc",
   "metadata": {},
   "source": [
    "Now, you can train a Transformer language model on the Penn Treebank dataset:\n",
    "\n",
    "Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d118e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n",
      "DEBUG: Using backend nd\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m corpus \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mCorpus(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ptb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m train_data \u001b[38;5;241m=\u001b[39m ndl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mbatchify(corpus\u001b[38;5;241m.\u001b[39mtrain, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_ptb(model, train_data, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mndl\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam)\n\u001b[1;32m     11\u001b[0m evaluate_ptb(model, train_data, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/mnt/zhumingzhu/work/00test/hw4_extra/./apps/models.py:38\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, embedding_size, output_size, hidden_size, num_layers, seq_model, seq_len, device, dtype)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28msuper\u001b[39m(LanguageModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cuda()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
    "model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=1, seq_model='transformer', seq_len=20, device=device)\n",
    "train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n",
    "evaluate_ptb(model, train_data, seq_len=20, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
